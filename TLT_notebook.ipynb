{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time object detection for disaster response using Transfer Learning Toolkit\n",
    "\n",
    "James Skinner, jskinner@nvidia.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Sign up for a free NVIDIA GPU Cloud (NGC) account at [ngc.nvidia.com](https://ngc.nvidia.com), then pull the Transfer Learning Toolkit (TLT) container from https://ngc.nvidia.com/catalog/containers/nvidia:tlt-streamanalytics\n",
    "\n",
    "Pull and enter the container\n",
    "\n",
    "    DATA_DIR=/path/to/your/data\n",
    "    WORKING_DIR=/path/to/workingdir # include the \"specs\" and \"deepstream\" directories\n",
    "    docker pull nvcr.io/nvidia/tlt-streamanalytics:v1.0_py2\n",
    "    docker run --runtime=nvidia -it -v $DATA_DIR:/data \\ \n",
    "        -v $WORKING_DIR:/src -p 8888:8888 \\ \n",
    "            nvcr.io/nvidia/tlt-streamanalytics:v1.0_py2 /bin/bash\n",
    "\n",
    "Configure TLT to use your NGC API key\n",
    "\n",
    "    ngc config set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set you NGC API key and some local directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY='<YOUR API KEY>'\n",
    "\n",
    "#Where we will save our data \n",
    "TLT_DIR='/data/tlt_working_dir'\n",
    "\n",
    "#Where out data is stored\n",
    "DATA_DIR='/data/stanford/kitti2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pre-trained model\n",
    "View models available on NGC.\n",
    "\n",
    "NB: If this doesn't work, did you run `ngc config set`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model list *detectnet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download your chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model download-version nvidia/iva/tlt_resnet50_detectnet_v2:1 -d $TLT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the contents of that folder, to check that a `.hdf5` file has been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $TLT_DIR/tlt_resnet50_detectnet_v2_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review data\n",
    "We are using the [Stanford Drones Dataset](http://cvgl.stanford.edu/projects/uav_data/). \n",
    "\n",
    "An extract from the raw data annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /data/stanford/annotations/coupa/video0/annotations.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each video is of a different resolution and duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ffmpeg -i /data/stanford/videos/coupa/video0/video.mov 2>&1 | grep Video: | grep -Po '\\d{3,5}x\\d{3,5}'\n",
    "ffprobe -i /data/stanford/videos/coupa/video0/video.mov 2>&1 -show_format | grep duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have pre-processed this by :\n",
    "1. Saving out every frame from each video, for example:\n",
    "\n",
    "        ffmpeg -i videos/bookstore/video0/video.mov -qscale:v 2 raw_jpgs/bookstore/bookstore_video0_%06d.jpg\n",
    "\n",
    "2. Randomly selecting some frames from each video\n",
    "\n",
    "        selected_frames = random.sample(frameslist, n_frames_per_vid)\n",
    "\n",
    "3. Randomly cropping each frame to 768 x 768.\n",
    "\n",
    "        im = Image.open(framepath)\n",
    "        width, height = im.size\n",
    "        # Select random crop\n",
    "        crop_xmin = random.randint(0, width - crop_w)\n",
    "        crop_ymin = random.randint(0, height - crop_h)\n",
    "        crop_xmax = crop_xmin + crop_w\n",
    "        crop_ymax = crop_ymin + crop_h\n",
    "\n",
    "\n",
    "4. Saving the annotations out in [KITTI format](https://docs.nvidia.com/metropolis/TLT/tlt-getting-started-guide/index.html#kitti_file)\n",
    "\n",
    "        kikki_output_list = [label, truncated, occluded, alpha, xmin, ymin, xmax, ymax, height_metres,\n",
    "                             width_metres, length_metres, cam_x, cam_y, cam_z, rot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a KITTI annotation file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $DATA_DIR/labels/coupa_video0_000047.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def display_image(img_path):\n",
    "    pil_im = Image.open(img_path) #Take jpg + png\n",
    "    im_array = np.asarray(pil_im)\n",
    "    plt.imshow(im_array)\n",
    "    plt.show()\n",
    "\n",
    "path = os.path.join(DATA_DIR, 'images/coupa_video0_000047.jpg')\n",
    "display(Image.open(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert dataset to TFRecords\n",
    "We convert our dataset into TFRecords using the `tlt-dataset-convert` command.\n",
    "\n",
    "We use a spec file to describe the dataset: [convert.txt](specs/convert.txt)\n",
    "\n",
    "**Change needed**: Be sure to update `root_directory_path` and `image_directory_path` to the location of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!more specs/convert.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make out TFRecords, creating the 15% validation split, as specified in convert.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-dataset-convert -d specs/convert.txt -o $TLT_DIR/tfrecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "We use a spec file to control the training process: [train.txt](specs/train.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, first make a directory in which to save your trained model.\n",
    "!mkdir $TLT_DIR/trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-train detectnet_v2 -e specs/train.txt \\\n",
    "        -r $TLT_DIR/trained --gpus 8 -k KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now find that your `trained` directory contains a number of models, named `model.step-xxx.tlt`, where for me `xxx` = 133080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $TLT_DIR/trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are going to use this model in several commands, let's save it as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=os.path.join(TLT_DIR, 'trained', 'model.step-133080.tlt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-evaluate detectnet_v2 -e specs/train.txt -m $MODEL -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I achieved the following accuracy:\n",
    "\n",
    "    class name      average precision (in %)\n",
    "    ------------  --------------------------\n",
    "    person                           43.9481\n",
    "    vehicle                          72.5628"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer\n",
    "We use a spec file to control the inference process: [infer.txt](specs/infer.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INFER_DIR='/data/stanford/kitti2/infer_imgs'\n",
    "OUTPUT_DIR='~/inferred_images'\n",
    "!tlt-infer detectnet_v2 -m $MODEL -i $INFER_DIR -o $OUTPUT_DIR -k $KEY -bs 16 -cp specs/infer.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune & retrain\n",
    "In the webinar, I didn't prune or re-train my model due to time constraints. The [Getting Started Guide](https://docs.nvidia.com/metropolis/TLT/tlt-getting-started-guide/index.html#pruning_models) contains information about the various pruning options.\n",
    "\n",
    "### Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $TLT_DIR/pruned\n",
    "!tlt-prune -pm $MODEL -o $TLT_DIR/pruned -pth 0.30 -nf 16 -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we trained the pruned model\n",
    "\n",
    "### Re-train\n",
    "\n",
    "We use a new spec file to control the training process: [retrain.txt](specs/retrain.txt)\n",
    "\n",
    "Make sure you change the `pretrained_model_file` to the model produced by the pruning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $TLT_DIR/pruned/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can re-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $TLT_DIR/retrained\n",
    "!tlt-train detectnet_v2 -e specs/retrain.txt \\\n",
    "        -r $TLT_DIR/retrained --gpus 8 -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to establish the best trained model and save that to a convenient variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $TLT_DIR/retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=os.path.join(TLT_DIR, 'retrained', 'model.step-133080.tlt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "You can export in any of FP32, FP16 or INT8 precision.\n",
    "\n",
    "### FP32\n",
    "\n",
    "### FP16\n",
    "\n",
    "    tlt-export $TLT_DIR/trained/model.step-133080.tlt -k $API_KEY \\\n",
    "        --export_module detectnet_v2 --outputs output_bbox/BiasAdd,output_cov/Sigmoid \\\n",
    "        --data_type fp16 --output_file $TLT_DIR/exports/FP16_model.etlt\n",
    "\n",
    "### INT8\n",
    "\n",
    "First generate the INT8 calibration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If necessary, make an export directory\n",
    "!mkdir $TLT_DIR/exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export with FP32 precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-export $MODEL -k $KEY \\\n",
    "    --export_module detectnet_v2 --outputs output_bbox/BiasAdd,output_cov/Sigmoid \\\n",
    "    --data_type fp32 --output_file $TLT_DIR/exports/FP32_model.etlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export with FP16 precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-export $MODEL -k $KEY \\\n",
    "    --export_module detectnet_v2 --outputs output_bbox/BiasAdd,output_cov/Sigmoid \\\n",
    "    --data_type fp16 --output_file $TLT_DIR/exports/FP16_model.etlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export with INT8 precision\n",
    "\n",
    "First we generate an INT8 calibration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-int8-tensorfile detectnet_v2 -e specs/train.txt \\\n",
    "        -o $TLT_DIR/exports/calibration.tensor -m 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-export $MODEL -k $KEY --export_module detectnet_v2 --outputs output_bbox/BiasAdd,output_cov/Sigmoid --data_type int8  --output_file $TLT_DIR/exports/INT8_model.etlt  --cal_data_file $TLT_DIR/exports/calibration.tensor --cal_cache_file $TLT_DIR/exports/calibration.bin --input_dims 3,768,768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review DeepStream config files\n",
    "\n",
    "The `.etlt` files above can be run directly in DeepStream using the `tlt-encoded-model` and `tlt-model-key` parameters.\n",
    "\n",
    "We are going to convert our `.etlt` models in TensorRT engines first, then use the config files below to run the model.\n",
    "\n",
    "* [labels.txt](deepstream/labels.txt)\n",
    "* [primary_inference.txt](deepstream/primary_inference.txt)\n",
    "    * This is looking for a file called `INT8_m1.plan`. We need to build this file on the Jetson device.\n",
    "* [stream_config.txt](deepstream/stream_config.txt)\n",
    "    * You need to replace `/path/to/your/mp4/video` with the path to your input video.\n",
    "\n",
    "## Copy files to edge device\n",
    "\n",
    "Now we move to the Jetson AGX Xavier to run our inference.\n",
    "\n",
    "    scp $TLT_DIR/exports/* <Jetson IP>:~/tlt\n",
    "    \n",
    "We also need to copy the DeepStream config files (discussed above) to a directory on the Jetson.\n",
    "\n",
    "    scp -r deepstream/* <Jetson IP>:~/tlt/ds_configs\n",
    "    \n",
    "## Run DeepStream (on Jetson)\n",
    "\n",
    "1. Download `tlt-converter` from developer.nvidia.com/transfer-learning-toolkit\n",
    "2. Convert your model to a TensorRT Engine. This creates the `INT8_m1.plan` file discussed above.\n",
    "\n",
    "        ./tlt-converter -k $KEY -d 3,768,768 \\\n",
    "        -o output_bbox/BiasAdd,output_cov/Sigmoid \\\n",
    "        -e ~/tlt/ds_configs/INT8_m1.plan \\\n",
    "        -t int8 \\\n",
    "        -c ~/tlt/calibration.bin \\\n",
    "        -m 1 \\\n",
    "        ~/tlt/INT8_model.etlt\n",
    "\n",
    "3. Change to the DeepStream samples directory\n",
    "\n",
    "        cd /opt/nvidia/deepstream/deepstream-4.0/samples\n",
    "    \n",
    "4. Run the stream!\n",
    "\n",
    "        deepstream-app -c ~/tlt/ds_configs/stream_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "**Developer zone**, to download `tlt-converter`: [developer.nvidia.com/transfer-learning-toolkit](https://developer.nvidia.com/transfer-learning-toolkit)\n",
    "\n",
    "**TLT getting started guide**: [docs.nvidia.com/metropolis/TLT/tlt-getting-started-guide/index.html](https://docs.nvidia.com/metropolis/TLT/tlt-getting-started-guide)\n",
    "\n",
    "**DeepStream webinar**: [info.nvidia.com/deepstream-to-improve-video-analytics-reg-page.html](https://info.nvidia.com/deepstream-to-improve-video-analytics-reg-page.html?ncid=so-lin-d2-97653&ondemandrgt=yes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}